{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with Azure AI Search and OpenAI\n",
    "\n",
    "This code demonstrates how to work with RAG to give more context to the LLM/SLM models to get a more accurate answer. The code uses Azure AI Search to index the documents and Azure OpenAI's embedding model to generate embeddings/vectors for the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install tiktoken\n",
    "%pip install azure-search-documents\n",
    "%pip install azure-identity\n",
    "%pip install openai\n",
    "%pip install PyPDF2\n",
    "%pip install python-docx\n",
    "%pip install pandas\n",
    "%pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Azure AI Search and OpenAI\n",
    "\n",
    "Load environment variables from the `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    load_dotenv(override=True)\n",
    "    config = dotenv_values(\".env\")\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_chat_completions_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETIONS_DEPLOYMENT_NAME\")\n",
    "\n",
    "azure_openai_embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "embedding_vector_dimensions = os.getenv(\"EMBEDDING_VECTOR_DIMENSIONS\")\n",
    "\n",
    "azure_search_service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "azure_search_service_admin_key = os.getenv(\"AZURE_SEARCH_SERVICE_ADMIN_KEY\")\n",
    "search_index_name = os.getenv(\"SEARCH_INDEX_NAME_1\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "# Test connection to OpenAI ChatGPT\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=azure_openai_chat_completions_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who are you ?\"}\n",
    "    ])\n",
    "print(completion.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of tokens in a text\n",
    "\n",
    "Like LLM models, Embedding models defines a `max input`. It is defined in number of `tokens`. The `max_input` for `text-embedding-3-large` is 8191 tokens. So we need to split the text into chunks of 8191 tokens or less. For that, you need to get the number of tokens in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name=\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string, disallowed_special=()))\n",
    "    return num_tokens\n",
    "\n",
    "# Test the function\n",
    "num_tokens_from_string(\"tiktoken is great!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI embedding model `text-embedding-3-large` has a limit of `8191` tokens per request.\n",
    "Before sending the files to the model, we need to split the text into chunks of less than `8191` tokens.\n",
    "Count the number of tokens in the sample files and show the files with more than `8191` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import docx\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def num_tokens_from_string(content):\n",
    "    # Implement or import your token counting logic here\n",
    "    return len(content.split())  # Example: Counting words as tokens\n",
    "\n",
    "input_directory = './data/myDocuments/'\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, filename)\n",
    "    content = ''\n",
    "\n",
    "    if filename.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in range(len(reader.pages)):\n",
    "                content += reader.pages[page].extract_text()\n",
    "\n",
    "    elif filename.endswith('.docx'):\n",
    "        doc = docx.Document(file_path)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            content += paragraph.text + '\\n'\n",
    "\n",
    "    elif filename.endswith('.csv'):\n",
    "        with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                content += ' '.join(row) + '\\n'\n",
    "\n",
    "    elif filename.endswith('.xlsx'):\n",
    "        df = pd.read_excel(file_path)\n",
    "        content += df.to_string(index=False)\n",
    "\n",
    "    # Add more elif statements if needed for .doc or other formats\n",
    "\n",
    "    tokens = num_tokens_from_string(content)\n",
    "    if tokens > 8191:\n",
    "        print(f'File {filename} has {tokens} tokens, which is more than 8191 (max) tokens.')\n",
    "    else:\n",
    "        print(f'File {filename} has {tokens} tokens.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Document Text\n",
    "Create a Function that will accept and extract text from various supported document types (such as .pdf, .docx, .csv, .xlsx, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import csv\n",
    "import docx\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from typing import Optional\n",
    "\n",
    "def extract_text_from_file(file_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract text content from a supported document file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str - The path to the document file.\n",
    "\n",
    "    Returns:\n",
    "    - Optional[str]: The extracted text content, or None if the file type is unsupported.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "    content = ''\n",
    "\n",
    "    # Extract text from a PDF file\n",
    "    if file_path.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                content += page.extract_text()\n",
    "\n",
    "    # Extract text from a DOCX file\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = docx.Document(file_path)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            content += paragraph.text\n",
    "\n",
    "    # Extract text from a CSV file\n",
    "    elif file_path.endswith('.csv'):\n",
    "        with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                content += ' '.join(row)\n",
    "\n",
    "    # Extract text from an Excel file\n",
    "    elif file_path.endswith('.xlsx'):\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            content += df.to_string(index=False) + '\\n\\n'\n",
    "\n",
    "    # Add more elif statements if needed for other file types (.doc, .txt, etc.)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "    return content.split('\\n\\n') if content else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a chunk titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming/cleaning the documents\n",
    "\n",
    "Functions that will need to remove all special characters and markdown syntax from the files. The function `clean_markdown_content()` will help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_markdown_content(content):\n",
    "    # Remove links\n",
    "    link_pattern = r'\\[([^\\[]+)\\]\\(([^\\)]+)\\)'\n",
    "    content = re.sub(link_pattern, r'\\1', content)\n",
    "\n",
    "    # Remove images\n",
    "    image_pattern = r'\\!\\[([^\\[]*)\\]\\(([^\\)]+)\\)'\n",
    "    content = re.sub(image_pattern, '', content)\n",
    "\n",
    "    # Remove all occurrences of **\n",
    "    content = content.replace('**', '')\n",
    "    content = content.replace('\\n', '')\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the vector embedding for an input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_vector(text):\n",
    "\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=azure_openai_embedding_model,\n",
    "    )\n",
    "\n",
    "    embedding = response.data[0].embedding\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# Test the function\n",
    "vector = get_embeddings_vector(\"Sample text\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create file chunks\n",
    "\n",
    "This is where we split the markdown files in folder `./data/myDocuments` into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import os\n",
    "import json\n",
    "\n",
    "input_directory = './data/myDocuments/'\n",
    "output_directory = './data/chunks/'\n",
    "suported_file_types = ('.pdf', '.docx', '.csv', '.xlsx')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "chunk_index = 0\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    # Check if the file is a PDF\n",
    "    if str(filename).endswith(suported_file_types):\n",
    "        # Extract the file's title (for example, using the filename)\n",
    "        page_title = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Open and read the PDF content\n",
    "        extracted_paragraphs = extract_text_from_file(input_directory + filename)\n",
    "\n",
    "        # Process each chunk\n",
    "        for chunk in extracted_paragraphs:\n",
    "            chunk_index += 1\n",
    "            chunk_content =  clean_markdown_content(chunk.strip())\n",
    "\n",
    "            if (num_tokens_from_string(chunk_content) > 8191):\n",
    "                    print(f'Chunk {chunk_index} in file {filename} has more than 8191 tokens')\n",
    "                    break\n",
    "            else:\n",
    "                print(f'Chunk {chunk_index} in file {filename} has {num_tokens_from_string(chunk_content)} tokens')\n",
    "\n",
    "            vector = get_embeddings_vector(chunk_content)\n",
    "\n",
    "            # Extract the chunk title using the first sentence or key content\n",
    "            chunk_title = chunk_content.split('.\\n')[0].strip()  # Assuming the first sentence ends with a period\n",
    "            if len(chunk_title) > 200:  # Limiting title length for practicality\n",
    "                chunk_title = chunk_title[:200] + '...'\n",
    "\n",
    "            chunk_data = {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                'page_title': page_title,\n",
    "                'chunk_title': chunk_title,  # The first line is the title of the chunk\n",
    "                'chunk_content': chunk_content,\n",
    "                'vector': vector\n",
    "            }\n",
    "            print(chunk_title)\n",
    "\n",
    "            chunk_file_name = f'chunk_{chunk_index}_{page_title}.json'.replace('?', '').replace(':', '').replace(\"'\", '').replace('|', '').replace('/', '').replace('\\\\', '')\n",
    "\n",
    "            # Write chunk into JSON file into output directory\n",
    "            with open(f'{output_directory}/{chunk_file_name}', 'w') as f:\n",
    "                json.dump(chunk_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the length of the embedding vector will be `1536` for `text-embedding-3-small` or `3072` for `text-embedding-3-large`. You can reduce the dimensions of the embedding by passing in the dimensions parameter without the embedding losing its concept-representing properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index in Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ComplexField,\n",
    "    CorsOptions,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    ScoringProfile,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SemanticField\n",
    ")\n",
    "\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "\n",
    "search_index_client = SearchIndexClient(\n",
    "    endpoint=azure_search_service_endpoint,\n",
    "    index_name=search_index_name,\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# create search index\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        sortable=True,\n",
    "        filterable=True,\n",
    "        facetable=True,\n",
    "    ),\n",
    "    SearchableField(name=\"page_title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"chunk_title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"chunk_content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=3072, #1536,\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"page_title\"),\n",
    "        # keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "        content_fields=[SemanticField(field_name=\"chunk_content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "# Create the search index with the semantic settings\n",
    "search_index = SearchIndex(name=search_index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = search_index_client.create_or_update_index(search_index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you ned to delete an index, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete index\n",
    "# search_index_client.delete_index(search_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload chunks/documents to Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, index_name=search_index_name, credential=credential)\n",
    "\n",
    "# for each json file in ./data/chunks/ folder, load the json document and upload it to the search index\n",
    "\n",
    "for filename in os.listdir(output_directory):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(output_directory, filename), 'r') as file:\n",
    "            document = json.load(file)\n",
    "\n",
    "            result = search_client.upload_documents(documents=document)\n",
    "            print(f\"Upload of {filename} succeeded: { result[0].succeeded }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search\n",
    "\n",
    "This example shows a pure vector search using the vectorizable text query, all you need to do is pass in text and your vectorizer will handle the query vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"iot\"\n",
    "\n",
    "embedding = get_embeddings_vector(query)\n",
    "\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"vector\")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=None,\n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"page_title\", \"chunk_title\", \"chunk_content\"],\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Page Title: {result['page_title']}\")\n",
    "    print(f\"Chunk Title: {result['chunk_title']}\")\n",
    "    print(f\"Chunk Content: {result['chunk_content']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a user query\n",
    "\n",
    "This is where we will use the Azure AI Search to search for documents similar to the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_query):\n",
    "\n",
    "    with open('safety_prompt.txt', 'r') as file:\n",
    "        safety_prompt = file.read()\n",
    "\n",
    "    SystemPrompt = \"You are a friendly and helpful assistant.\"+ safety_prompt\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=azure_openai_chat_completions_deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SystemPrompt},\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        extra_body={\n",
    "            \"data_sources\": [\n",
    "                {\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": azure_search_service_endpoint,\n",
    "                        \"index_name\": search_index_name,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": azure_search_service_admin_key,\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = input(\"Enter your question: \")\n",
    "response = get_response(user_query)\n",
    "print(response.to_json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
